---
author: "Centre for Population Genomics"
date: "`r Sys.time()`"
output:
  html_document:
    theme: cosmo
    css: style.css
    toc: true
    code_download: true
    code_folding: show
  rmdformats::material:
    highlight: kate
params:
  title: "HGDP Metadata Exploration"
description: "HGDP Metadata Exploration"
title: "`r paste(params$title)`"
---

```{r knitr_opts, include=F}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

```{r load_pkgs}
require(tidyverse)
require(fs)
require(reactable)
require(glue)
require(janitor)
require(downloadthis)

source("functions.R")
```

## Introduction

Here we're exploring CRAM files and metadata associated with the
Human Genome Diversity Project
([HGDP](https://www.internationalgenome.org/data-portal/data-collection/hgdp)).

### Contents of `ibd-external-datasets/HGDP/broad_reprocessed_crams`

- List of files generated in 2021-Mar-12 with:

```text
gsutil -u <project> gs://ibd-external-datasets/HGDP/broad_reprocessed_crams
```

```{r read_cram_list}
theme_set(theme_bw())
cram_plus_qc <-
  "data/cram_plus_qc_list.txt" %>%
  readr::read_table(col_names = c("size", "fname"), col_types = "cc") %>%
  dplyr::mutate(bname = basename(fname),
                ftype = guess_file_type(fname),
                size = fs::as_fs_bytes(size))

cram_plus_qc %>%
  dplyr::count(ftype) %>% knitr::kable(caption = "Type of files in bucket.") # 948 of each

# Keep the CRAMs and grab the sample name from the file name
cram <- cram_plus_qc %>%
  dplyr::filter(ftype == "CRAM") %>%
  dplyr::mutate(sample_name = dplyr::case_when(
    grepl("LP600", bname) ~ sub("(LP600.*)\\.srt.aln.cram", "\\1", bname),
    grepl("SS600", bname) ~ sub("(SS600.*)\\.srt.aln.cram", "\\1", bname),
    grepl("HGDP0", bname) ~ sub("(HGDP0.*)\\.alt_bwamem_GRCh38DH.*", "\\1", bname),
    TRUE ~ bname)) %>%
  dplyr::select(sample_name, fname, size)
cram
```

- Total of **`r nrow(cram_plus_qc)`** files, consisting of **`r nrow(cram)`**
  sets of CRAMs, QC metrics, md5sums etc.
- The QC metrics are actually summarised in a file linked in
  <https://github.com/atgu/hgdp_tgp> by the MGH ATGU team (**explored in the
  [ATGU metadata](#atgu-metadata) section of this report**).
- Let's check out the CRAM file sizes:

```{r cram_file_sizes}
# ~15TB of CRAMs
cram %>%
  dplyr::summarise(mean = fs::as_fs_bytes(mean(size)),
                   q1 = fs::as_fs_bytes(quantile(size, 0.25)),
                   median = fs::as_fs_bytes(median(size)),
                   q3 = fs::as_fs_bytes(quantile(size, 0.75)),
                   max = fs::as_fs_bytes(max(size)),
                   total = fs::as_fs_bytes(sum(size)),
                   .groups = "drop") %>%
  tidyr::pivot_longer(cols = mean:total) %>%
  knitr::kable(caption = "CRAM file size metrics.")

ggplot(cram, aes(x = size)) +
  geom_histogram(bins = 40, fill='#aec7e8', color='#1f77b4') +
  scale_x_continuous(labels = scales::comma, breaks = scales::breaks_pretty(10)) +
  scale_y_continuous(breaks = scales::breaks_pretty(10)) +
  ggtitle(glue::glue("CRAM file size (bytes) for {nrow(cram)} samples."))
```

### ATGU metadata {#atgu-metadata}

- Let's annotate these samples with metadata linked in Google Drive
  from <https://github.com/atgu/hgdp_tgp> (2021-Mar-12),
  after joining with above tibble:

```{r gnomad_metadata}
gnomad <-
  "data/gnomad_meta_hgdp_tgp_v1.txt" %>%
  readr::read_tsv() %>%
  janitor::remove_empty("cols") # down to 137 from 184 columns

# show example columns
cnames <- dplyr::tibble(column = colnames(gnomad),
                        male_ex = t(gnomad[1, ]),
                        female_ex = t(gnomad[80, ])) %>%
  reactable::reactable(
    pagination = FALSE, highlight = TRUE, height = 650,
    searchable = TRUE, filterable = TRUE, bordered = TRUE)

htmlwidgets::prependContent(
  cnames,
  htmltools::h2(class = "title", "Metadata column names"))

# select a few of the main qc columns and join with cram
d <- gnomad %>%
  dplyr::rename(sample_name = project_meta.sample_id) %>%
  dplyr::right_join(cram, by = "sample_name") %>%
  dplyr::arrange(sample_name) %>%
  dplyr::select(sample_name, size, sex = project_meta.sex, high_quality,
                cov_med = bam_metrics.median_coverage,
                pop = project_meta.project_pop,
                subpop = project_meta.project_subpop,
                pop_inf = population_inference.pop,
                base10x = bam_metrics.pct_bases_10x,
                base20x = bam_metrics.pct_bases_20x,
                freemix = bam_metrics.freemix,
                insert_size_med = bam_metrics.median_insert_size,
                n_snp = sample_qc.n_snp, fname)
```

### Filter1

- Let's look at samples that have call rate >= 0.895,
  and median depth between 30 and 40 (pre-filtered from [gnomAD 1KGP + HGDP
  MatrixTable](https://gnomad.broadinstitute.org/downloads)):

```{r read_filtered_matrixtable}
# 597 total
hq_samples <- "data/filtered_table_kat.txt" %>%
  readr::read_tsv(col_types = cols_only(s = col_character())) %>%
  dplyr::pull()

# all are in the bucket
table(hq_samples %in% d$sample_name)

# Include 'oth' pop explicitly (papuan/melanesian) - these get filtered
# out otherwise. Total: 627
d_filt1 <- d %>%
  dplyr::filter(sample_name %in% hq_samples | pop %in% "oth") %>%
  dplyr::mutate(
    pop_long = dplyr::case_when(
      pop == "afr" ~ "Africa",
      pop == "amr" ~ "America",
      pop == "sas" ~ "Central South Asia",
      pop == "eas" ~ "East Asia",
      pop == "nfe" ~ "Europe (non-Finnish)",
      pop == "mid" ~ "Middle East",
      pop == "oth" ~ "Other",
      TRUE ~ "UNKNOWN")) %>%
  dplyr::arrange(sample_name)
```

### Filter2

- Total of **`r nrow(d_filt1)`** after filtration by call rate & coverage (but
  including Papuan/Melanesian samples even if they failed those filters).
- Now that we're confident we have good quality data (except the
  Papuan/Melanesian samples), we can randomly pick 4 male and 4 female samples
  across the seven different populations. Let's also remove any samples with
  a CRAM file larger than 18G:

- America: Mayan
- Central South Asia: Hazara, Macrani
- East Asia: Cambodia, Uygur, Yizu, Han, She, Yakut
- Europe: Basque, Russian
- Middle East: Bedouin


```{r}
set.seed(123)
d_filt1 %>%
  count(pop_long, sex) %>%
  knitr::kable(caption = "Count of males/females per population.")

d_filt2 <- d_filt1 %>%
  dplyr::filter(size < fs::as_fs_bytes("18G")) %>%
  dplyr::group_by(pop_long, sex) %>%
  dplyr::sample_n(4)

to_add <- c("HGDP00099", "HGDP00607", "HGDP00716", "HGDP00774", "HGDP00854",
            "HGDP00879", "HGDP00946", "HGDP01181", "HGDP01301", "HGDP01327",
            "HGDP01357", "HGDP00858", "HGDP00618")

to_remove <- c("LP6005441-DNA_E10", "HGDP01059", "HGDP00733", "HGDP00740")

d_filt3 <- d_filt1 %>%
  dplyr::filter(sample_name %in% to_add)

d_filt4 <- d_filt2 %>%
  dplyr::filter(!sample_name %in% to_remove)

d_final1 <- dplyr::bind_rows(d_filt3, d_filt4)
d_final1 %>% count(pop_long, sex) %>% knitr::kable()
```

```{r plots}
ggplot(d_final1, aes(x = size)) +
  geom_histogram(bins = 40, fill='#aec7e8', color='#1f77b4') +
  scale_x_continuous(labels = scales::comma,
                     breaks = scales::breaks_pretty(10)) +
  scale_y_continuous(breaks = scales::breaks_pretty(10)) +
  coord_flip() +
  ggtitle(glue("CRAM file size in bytes (for {nrow(d_final1)} samples)."))

ggplot(d_final1, aes(x = "", y = cov_med)) +
  geom_boxplot() +
  geom_jitter(shape = 21, colour = '#008080', fill = "#1f77b4",
              width = 0.35, height = 0.08) +
  coord_flip() +
  scale_y_continuous(breaks = scales::breaks_pretty(8)) +
  ggtitle(glue("Median coverage (for {nrow(d_final1)} samples).")) +
  xlab("")

ggplot(d_final1, aes(x = base20x)) +
  geom_histogram(bins = 40, fill='#aec7e8', color='#1f77b4') +
  scale_x_continuous(labels = scales::comma,
                     breaks = scales::breaks_pretty(10)) +
  ggtitle(glue("% bases covered by at least 20 reads ({nrow(d_final1)} samples)"))

ggplot(d_final1, aes(x = base10x)) +
  geom_histogram(bins = 40, fill='#aec7e8', color='#1f77b4') +
  scale_x_continuous(labels = scales::comma,
                     breaks = scales::breaks_pretty(10)) +
  ggtitle(glue("% bases covered by at least 10 reads ({nrow(d_final1)} samples)"))

ggplot(d_final1, aes(x = n_snp)) +
  geom_histogram(bins = 40, fill='#aec7e8', color='#1f77b4') +
  scale_x_continuous(labels = scales::comma,
                     breaks = scales::breaks_pretty(10)) +
  ggtitle(glue("Number of SNPs ({nrow(d_final1)} samples)"))

ggplot(d_final1, aes(x = insert_size_med)) +
  geom_histogram(bins = 40, fill='#aec7e8', color='#1f77b4') +
  scale_x_continuous(labels = scales::comma,
                     breaks = scales::breaks_pretty(10)) +
  ggtitle(glue("Median insert size ({nrow(d_final1)} samples)"))
```

## Final sample table

- Total of:
  - **`r nrow(d_final1)`** samples
  - **`r options(scipen=999); as.character(fs::as_fs_bytes(sum(d_final1$size)))`** CRAM size

```{r table_all}
d_final2 <- d_final1 %>%
  dplyr::mutate(size_cram = as.character(size)) %>%
  dplyr::select(sample_name, pop_long, subpop, sex, cov_med, base10x, base20x,
                freemix, insert_size_med, n_snp, size_cram, fname)

download_button <- d_final2 %>%
  downloadthis::download_this(
    output_name = "hgdp_final_56samples",
    output_extension = ".csv",
    button_label = "Download CSV Table",
    button_type = "primary",
    has_icon = TRUE,
    icon = "fa fa-save"
  )

tab <- d_final2 %>%
  dplyr::select(-c(fname)) %>%
  reactable::reactable(
    pagination = FALSE, highlight = TRUE, height = 2000,
    searchable = TRUE, filterable = TRUE, groupBy = c("pop_long"),
    bordered = TRUE,
    columns = list(
      subpop = colDef(aggregate = "frequency"),
      sex = colDef(aggregate = "frequency")
     ))
```

```{r echo=FALSE}
download_button
tab
```
